{
  "assume_beta": "@model function assume_beta()\n    a ~ Beta(2, 2)\nend\n\nmodel = assume_beta()",
  "assume_dirichlet": "@model function assume_dirichlet()\n    a ~ Dirichlet([1.0, 5.0])\nend\n\nmodel = assume_dirichlet()",
  "demo_assume_dot_observe": "@model function demo_assume_dot_observe(x = [1.5, 2.0])\n    # `assume` and `dot_observe`\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    x .~ Normal(m, sqrt(s))\nend\n\nmodel = demo_assume_dot_observe()",
  "demo_assume_dot_observe_literal": "@model function demo_assume_dot_observe_literal()\n    # `assume` and literal `dot_observe`\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    [1.5, 2.0] .~ Normal(m, sqrt(s))\nend\n\nmodel = demo_assume_dot_observe_literal()",
  "demo_assume_index_observe": "using LinearAlgebra: Diagonal\n\n@model function demo_assume_index_observe(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `assume` with indexing and `observe`\n    s = TV(undef, length(x))\n    for i in eachindex(s)\n        s[i] ~ InverseGamma(2, 3)\n    end\n    m = TV(undef, length(x))\n    for i in eachindex(m)\n        m[i] ~ Normal(0, sqrt(s[i]))\n    end\n    x ~ MvNormal(m, Diagonal(s))\nend\n\nmodel = demo_assume_index_observe()",
  "demo_assume_matrix_observe_matrix_index": "using LinearAlgebra: Diagonal\n\n@model function demo_assume_matrix_observe_matrix_index(\n    x = transpose([1.5 2.0;]),\n    ::Type{TV} = Array{Float64},\n) where {TV}\n    n = length(x)\n    d = n \u00f7 2\n    s ~ reshape(product_distribution(fill(InverseGamma(2, 3), n)), d, 2)\n    s_vec = vec(s)\n    m ~ MvNormal(zeros(n), Diagonal(s_vec))\n    x[:, 1] ~ MvNormal(m, Diagonal(s_vec))\nend\n\nmodel = demo_assume_matrix_observe_matrix_index()",
  "demo_assume_multivariate_observe": "using LinearAlgebra: Diagonal\n\n@model function demo_assume_multivariate_observe(x = [1.5, 2.0])\n    # Multivariate `assume` and `observe`\n    s ~ product_distribution([InverseGamma(2, 3), InverseGamma(2, 3)])\n    m ~ MvNormal(zero(x), Diagonal(s))\n    x ~ MvNormal(m, Diagonal(s))\nend\n\nmodel = demo_assume_multivariate_observe()",
  "demo_assume_multivariate_observe_literal": "using LinearAlgebra: Diagonal\n\n@model function demo_assume_multivariate_observe_literal()\n    # multivariate `assume` and literal `observe`\n    s ~ product_distribution([InverseGamma(2, 3), InverseGamma(2, 3)])\n    m ~ MvNormal(zeros(2), Diagonal(s))\n    [1.5, 2.0] ~ MvNormal(m, Diagonal(s))\nend\n\nmodel = demo_assume_multivariate_observe_literal()",
  "demo_assume_observe_literal": "@model function demo_assume_observe_literal()\n    # univariate `assume` and literal `observe`\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    1.5 ~ Normal(m, sqrt(s))\n    2.0 ~ Normal(m, sqrt(s))\nend\n\nmodel = demo_assume_observe_literal()",
  "demo_assume_submodel_observe_index_literal": "@model function _prior_dot_assume(::Type{TV} = Vector{Float64}) where {TV}\n    s = TV(undef, 2)\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, 2)\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    return s, m\nend\n\n@model function demo_assume_submodel_observe_index_literal()\n    # Submodel prior\n    priors ~ to_submodel(_prior_dot_assume(), false)\n    s, m = priors\n    1.5 ~ Normal(m[1], sqrt(s[1]))\n    2.0 ~ Normal(m[2], sqrt(s[2]))\nend\n\nmodel = demo_assume_submodel_observe_index_literal()",
  "demo_dot_assume_observe": "using LinearAlgebra: Diagonal\n\n@model function demo_dot_assume_observe(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `dot_assume` and `observe`\n    s = TV(undef, length(x))\n    m = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    x ~ MvNormal(m, Diagonal(s))\nend\n\nmodel = demo_dot_assume_observe()",
  "demo_dot_assume_observe_index": "@model function demo_dot_assume_observe_index(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `dot_assume` and `observe` with indexing\n    s = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, length(x))\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    for i in eachindex(x)\n        x[i] ~ Normal(m[i], sqrt(s[i]))\n    end\nend\n\nmodel = demo_dot_assume_observe_index()",
  "assume_lkjcholu": "@model function assume_lkjcholu()\n    a ~ LKJCholesky(5, 1.0, 'U')\nend\n\nmodel = assume_lkjcholu()",
  "demo_dot_assume_observe_index_literal": "@model function demo_dot_assume_observe_index_literal(\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `dot_assume` and literal `observe` with indexing\n    s = TV(undef, 2)\n    m = TV(undef, 2)\n    s .~ InverseGamma(2, 3)\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n\n    1.5 ~ Normal(m[1], sqrt(s[1]))\n    2.0 ~ Normal(m[2], sqrt(s[2]))\nend\n\nmodel = demo_dot_assume_observe_index_literal()",
  "demo_dot_assume_observe_matrix_index": "using LinearAlgebra: Diagonal\n\n@model function demo_dot_assume_observe_matrix_index(\n    x = transpose([1.5 2.0;]),\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    s = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, length(x))\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    x[:, 1] ~ MvNormal(m, Diagonal(s))\nend\n\nmodel = demo_dot_assume_observe_matrix_index()",
  "demo_dot_assume_observe_submodel": "using LinearAlgebra: Diagonal\n\n@model function _likelihood_multivariate_observe(s, m, x)\n    return x ~ MvNormal(m, Diagonal(s))\nend\n\n@model function demo_dot_assume_observe_submodel(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    s = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, length(x))\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n\n    # Submodel likelihood\n    # With to_submodel, we have to have a left-hand side variable to\n    # capture the result, so we just use a dummy variable\n    _ignore ~ to_submodel(_likelihood_multivariate_observe(s, m, x))\nend\n\nmodel = demo_dot_assume_observe_submodel()",
  "dot_assume": "@model function dot_assume(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 5)\n    a .~ Normal()\nend\n\nmodel = dot_assume()",
  "dot_observe": "@model function dot_observe(x = [1.5, 2.0, 2.5])\n    a ~ Normal()\n    x .~ Normal(a)\nend\n\nmodel = dot_observe()",
  "dppl_gauss_unknown": "n = 10_000\ns = abs(rand()) + 0.5\ny = randn() .+ s * randn(n)\n\n@model function dppl_gauss_unknown(y)\n    N = length(y)\n    m ~ Normal(0, 1)\n    s ~ truncated(Cauchy(0, 5); lower=0)\n    y ~ product_distribution(fill(Normal(m, s), N))\nend\n\nmodel = dppl_gauss_unknown(y)",
  "dppl_hier_poisson": "using Turing: LogPoisson\n\nnd, ns = 5, 10\na0, a1, a0_sig = 1.0, 0.5, 0.3\nn = nd * ns\n# simulate group level parameters\na0s = rand(Normal(0, a0_sig), ns)\nlogpop = rand(Normal(9, 1.5), ns)\n\u03bb = exp.(a0 .+ a0s + (a1 * logpop))\n# and individual data\ny = mapreduce(\u03bbi -> rand(Poisson(\u03bbi), nd), vcat, \u03bb)\nx = repeat(logpop, inner=nd)\nidx = repeat(collect(1:ns), inner=nd)\n\n@model function dppl_hier_poisson(y, x, idx, ns)\n    a0 ~ Normal(0, 10)\n    a1 ~ Normal(0, 1)\n    a0_sig ~ truncated(Cauchy(0, 1); lower=0)\n    a0s ~ product_distribution(fill(Normal(0, a0_sig), ns))\n    alpha = a0 .+ a0s[idx] .+ a1 * x\n    y ~ product_distribution(LogPoisson.(alpha))\nend\n\nmodel = dppl_hier_poisson(y, x, idx, ns)",
  "dppl_high_dim_gauss": "@model function dppl_high_dim_gauss(D)\n    m ~ product_distribution(fill(Normal(0, 1), D))\nend\n\nmodel = dppl_high_dim_gauss(10_000)",
  "dppl_hmm_semisup": "using StatsFuns: logsumexp\n\n# Set up hyperparameters\nK, v, T, T_unsup = 5, 20, 100, 200\nalpha = fill(1.0, K)\nbeta = fill(0.1, v)\ntheta = rand(Dirichlet(alpha), K)\nphi = rand(Dirichlet(beta), K)\n\n# Simulate data (supervised)\nw = Vector{Int}(undef, T)\nz = Vector{Int}(undef, T)\nz[1] = rand(1:K)\nw[1] = rand(Categorical(phi[:, z[1]]))\nfor t in 2:T\n    z[t] = rand(Categorical(theta[:, z[t - 1]]))\n    w[t] = rand(Categorical(phi[:, z[t]]))\nend\n\n# Unsupervised\nu = Vector{Int}(undef, T_unsup)\ny = Vector{Int}(undef, T_unsup)\ny[1] = rand(1:K)\nu[1] = rand(Categorical(phi[:, y[1]]))\nfor t in 2:T_unsup\n    y[t] = rand(Categorical(theta[:, y[t - 1]]))\n    u[t] = rand(Categorical(phi[:, y[t]]))\nend\n\n@model function dppl_hmm_semisup(K, T, T_unsup, w, z, u, alpha, beta)\n    theta ~ product_distribution(fill(Dirichlet(alpha), K))\n    phi ~ product_distribution(fill(Dirichlet(beta), K))\n    for t in 1:T\n        w[t] ~ Categorical(phi[:, z[t]]);\n    end\n    for t in 2:T\n        z[t] ~ Categorical(theta[:, z[t - 1]]);\n    end\n\n    TF = eltype(theta)\n    acc = similar(alpha, TF, K)\n    gamma = similar(alpha, TF, K)\n    temp_gamma = similar(alpha, TF, K)\n    for k in 1:K\n        gamma[k] = log(phi[u[1],k])\n    end\n    for t in 2:T_unsup\n        for k in 1:K\n            for j in 1:K\n                acc[j] = gamma[j] + log(theta[k, j]) + log(phi[u[t], k])\n            end\n            temp_gamma[k] = logsumexp(acc)\n        end\n        gamma .= temp_gamma\n    end\n    @addlogprob! logsumexp(gamma)\nend\n\nmodel = dppl_hmm_semisup(K, T, T_unsup, w, z, u, alpha, beta)",
  "dppl_lda": "v = 100      # words\nk = 5        # topics\nm = 10       # number of docs\nalpha = ones(k)\nbeta = ones(v)\n\nphi = rand(Dirichlet(beta), k)\ntheta = rand(Dirichlet(alpha), m)\ndoc_lengths = rand(Poisson(1_000), m)\nn = sum(doc_lengths)\n\nw = Vector{Int}(undef, n)\ndoc = Vector{Int}(undef, n)\nfor i in 1:m\n    local idx = sum(doc_lengths[1:i-1]) # starting index for inner loop\n    for j in 1:doc_lengths[i]\n        z = rand(Categorical(theta[:, i]))\n        w[idx + j] = rand(Categorical(phi[:, z]))\n        doc[idx + j] = i\n    end\nend\n\n@model function dppl_lda(k, m, w, doc, alpha, beta)\n    theta ~ product_distribution(fill(Dirichlet(alpha), m))\n    phi ~ product_distribution(fill(Dirichlet(beta), k))\n    log_phi_dot_theta = log.(phi * theta)\n    @addlogprob! sum(log_phi_dot_theta[CartesianIndex.(w, doc)])\nend\n\nmodel = dppl_lda(k, m, w, doc, alpha, beta)",
  "assume_mvnormal": "@model function assume_mvnormal()\n    a ~ MvNormal([0.0, 0.0], [1.0 0.5; 0.5 1.0])\nend\n\nmodel = assume_mvnormal()",
  "dppl_logistic_regression": "using StatsFuns: logistic\n\nfunction safelogistic(x::T) where {T}\n    logistic(x) * (1 - 2 * eps(T)) + eps(T)\nend\n\nd, n = 100, 10_000\nX = randn(d, n)\nw = randn(d)\ny = Int.(logistic.(X' * w) .> 0.5)\n\n\n@model function dppl_logistic_regression(Xt, y)\n    N, D = size(Xt)\n    w ~ product_distribution(Normal.(zeros(D)))\n    y ~ product_distribution(Bernoulli.(safelogistic.(Xt * w)))\nend\n\nmodel = dppl_logistic_regression(X', y)",
  "dppl_naive_bayes": "using MLDatasets: MNIST\nusing MultivariateStats: fit, PCA, transform\n\n# Load MNIST images and labels\nfeatures = MNIST(split=:train).features\nnrows, ncols, nimages = size(features)\nimage_raw = Float64.(reshape(features, (nrows * ncols, nimages)))\nlabels = MNIST(split=:train).targets .+ 1\nC = 10 # Number of labels\n\n# Preprocess the images by reducing dimensionality\nD = 40\npca = fit(PCA, image_raw; maxoutdim=D)\nimage = transform(pca, image_raw)\n\n# Take only the first 1000 images and vectorise\nN = 1000\nimage_subset = image[:, 1:N]'\nimage_vec = image_subset[:, :]\nlabels = labels[1:N]\n\n@model function dppl_naive_bayes(image_vec, labels, C, D)\n    m ~ product_distribution(fill(Normal(0, 10), C, D))\n    image_vec ~ product_distribution(Normal.(m[labels, :]))\nend\n\nmodel = dppl_naive_bayes(image_vec, labels, C, D)",
  "dppl_sto_volatility": "using DelimitedFiles: readdlm\n\npath = \"$(@__DIR__)/../data/dppl_sto_volatility.csv\"\ndata, _ = readdlm(path, ',', header=true)\nto_num(x) = x isa Number ? x : 0.1\ny = map(to_num, data[1:500, 2])\n\n@model function dppl_sto_volatility(y, ::Type{Tv}=Vector{Float64}) where {Tv}\n    T = length(y)\n    \u03bc ~ Cauchy(0, 10)\n    \u03d5 ~ Uniform(-1, 1)\n    \u03c3 ~ truncated(Cauchy(0, 5); lower=0)\n\n    h = Tv(undef, T)\n    h[1] ~ Normal(\u03bc, \u03c3 / sqrt(1 - \u03d5^2))\n    y[1] ~ Normal(0, exp(h[1] / 2))\n    for t in 2:T\n        h[t] ~ Normal(\u03bc + \u03d5 * (h[t-1] - \u03bc), \u03c3)\n        y[t] ~ Normal(0, exp(h[t] / 2))\n    end\nend\n\nmodel = dppl_sto_volatility(y)",
  "dynamic_constraint": "@model function dynamic_constraint()\n    a ~ Normal()\n    b ~ truncated(Normal(); lower = a)\nend\n\nmodel = dynamic_constraint()",
  "metabayesian_MH": "#=\nThis is a \"meta-Bayesian\" model, where the generative model includes an inversion of a different generative model.\nThese types of models are common in cognitive modelling, where systems of interest (e.g. human subjects) are thought to use Bayesian inference to navigate their environment.\nHere we use a Metropolis-Hasting sampler implemented with Turing as the inversion of the inner \"subjective\" model.\n=#\nusing Random: Xoshiro\n\n# Inner model function\n@model function inner_model(observation, prior_\u03bc = 0, prior_\u03c3 = 1)\n    # The inner model's prior\n    mean ~ Normal(prior_\u03bc, prior_\u03c3)\n    # The inner model's likelihood\n    observation ~ Normal(mean, 1)\nend\n\n# Outer model function\n@model function metabayesian_MH(\n    observation,\n    action,\n    inner_sampler = MH(),\n    inner_n_samples = 20,\n)\n    ### Sample parameters for the inner inference and response ###\n    # The inner model's prior's sufficient statistics\n    subj_prior_\u03bc ~ Normal(0, 1)\n    subj_prior_\u03c3 = 1.0\n    # Inverse temperature for actions\n    \u03b2 ~ Exponential(1)\n\n    ### \"Perceptual inference\": running the inner model ###\n    # Condition the inner model\n    inner_m = inner_model(observation, subj_prior_\u03bc, subj_prior_\u03c3)\n    # Run the inner Bayesian inference\n    chns = sample(Xoshiro(468), inner_m, inner_sampler, inner_n_samples, progress = false)\n    # Extract subjective point estimate\n    subj_mean_expectation\u209c = mean(chns[:mean])\n\n\n    ### \"Response model\": picking an action ###\n    # The action is a Gaussian-noise report of the subjective point estimate\n    action ~ Normal(subj_mean_expectation\u209c, \u03b2)\nend\n\nmodel = metabayesian_MH(0.0, 1.0)",
  "multiple_constraints_same_var": "@model function multiple_constraints_same_var(::Type{TV} = Vector{Float64}) where {TV}\n    x = TV(undef, 5)\n    x[1] ~ Normal()\n    x[2] ~ InverseGamma(2, 3)\n    x[3] ~ truncated(Normal(), -5, 20)\n    x[4:5] ~ Dirichlet([1.0, 2.0])\nend\n\nmodel = multiple_constraints_same_var()",
  "multithreaded": "#=\nMost models in ADTests are run with 1 thread. This model is run with 2 threads\nto properly demonstrate the compatibility with multithreaded observe\nstatements. See `main.jl` for more information.\n=#\n\n@model function multithreaded(x)\n    a ~ Normal()\n    Threads.@threads for i in eachindex(x)\n        x[i] ~ Normal(a)\n    end\nend\n\nmodel = multithreaded([1.5, 2.0, 2.5, 1.5, 2.0, 2.5])",
  "n010": "@model function n010(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 10)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\nmodel = n010()",
  "n050": "@model function n050(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 50)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\nmodel = n050()",
  "n100": "@model function n100(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 100)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\nmodel = n100()",
  "assume_normal": "@model function assume_normal()\n    a ~ Normal()\nend\n\nmodel = assume_normal()",
  "n500": "@model function n500(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 500)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\nmodel = n500()",
  "observe_bernoulli": "@model function observe_bernoulli(x = [true, false, true])\n    a ~ Beta(2, 2)\n    for i in eachindex(x)\n        x[i] ~ Bernoulli(a)\n    end\nend\n\nmodel = observe_bernoulli()",
  "observe_categorical": "@model function observe_categorical(x = [1, 2, 1, 2, 2])\n    a ~ Dirichlet(2, 3)\n    for i in eachindex(x)\n        x[i] ~ Categorical(a)\n    end\nend\n\nmodel = observe_categorical()",
  "observe_index": "@model function observe_index(x = [1.5, 2.0, 2.5])\n    a ~ Normal()\n    for i in eachindex(x)\n        x[i] ~ Normal(a)\n    end\nend\n\nmodel = observe_index()",
  "observe_literal": "@model function observe_literal()\n    a ~ Normal()\n    1.5 ~ Normal(a)\nend\n\nmodel = observe_literal()",
  "observe_multivariate": "@model function observe_multivariate(\n    x = [1.5, 2.0, 2.5],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    a = TV(undef, length(x))\n    a .~ Normal()\n    x ~ MvNormal(a, I)\nend\n\nmodel = observe_multivariate()",
  "observe_submodel": "@model function inner2(x, a)\n    x ~ Normal(a)\nend\n@model function observe_submodel(x = 1.5)\n    a ~ Normal()\n    _ignore ~ to_submodel(inner2(x, a))\nend\n\nmodel = observe_submodel()",
  "observe_von_mises": "@model function observe_von_mises(x)\n    a ~ InverseGamma(2, 3)\n    x ~ VonMises(0, a)\nend\n\nmodel = observe_von_mises(0.4)",
  "ordinary_diffeq": "# See https://turinglang.org/docs/tutorials/bayesian-differential-equations/.\n\nusing OrdinaryDiffEq: ODEProblem, solve, Tsit5\n\nfunction lotka_volterra(du, u, p, t)\n    \u03b1, \u03b2, \u03b3, \u03b4 = p\n    x, y = u\n    du[1] = (\u03b1 - \u03b2 * y) * x # prey\n    du[2] = (\u03b4 * x - \u03b3) * y # predator\n    return nothing\nend\nu0 = [1.0, 1.0]\np = [1.5, 1.0, 3.0, 1.0]\ntspan = (0.0, 10.0)\nprob = ODEProblem(lotka_volterra, u0, tspan, p)\nsol = solve(prob, Tsit5(); saveat=0.1)\nq = 1.7\nodedata = rand.(Poisson.(q * Array(sol)))\n\n@model function ordinary_diffeq(data, prob)\n    \u03b1 ~ truncated(Normal(1.5, 0.2); lower=0.5, upper=2.5)\n    \u03b2 ~ truncated(Normal(1.1, 0.2); lower=0, upper=2)\n    \u03b3 ~ truncated(Normal(3.0, 0.2); lower=1, upper=4)\n    \u03b4 ~ truncated(Normal(1.0, 0.2); lower=0, upper=2)\n    q ~ truncated(Normal(1.7, 0.2); lower=0, upper=3)\n    p = [\u03b1, \u03b2, \u03b3, \u03b4]\n    predicted = solve(prob, Tsit5(); p=p, saveat=0.1, abstol=1e-6, reltol=1e-6)\n    for i in eachindex(predicted)\n        data[:, i] ~ product_distribution(Poisson.(q .* predicted[i] .+ 1e-5))\n    end\n    return nothing\nend\n\nmodel = ordinary_diffeq(odedata, prob)",
  "pdb_eight_schools_centered": "J = 8\ny = [28, 8, -3, 7, -1, 1, 18, 12]\nsigma = [15, 10, 16, 11, 9, 11, 10, 18]\n\n@model function pdb_eight_schools_centered(J, y, sigma)\n    mu ~ Normal(0, 5)\n    tau ~ truncated(Cauchy(0, 5); lower = 0)\n    theta = Vector{Float64}(undef, J)\n    for i = 1:J\n        theta[i] ~ Normal(mu, tau)\n        y[i] ~ Normal(theta[i], sigma[i])\n    end\nend\n\nmodel = pdb_eight_schools_centered(J, y, sigma)",
  "assume_submodel": "@model function inner1()\n    return a ~ Normal()\nend\n@model function assume_submodel()\n    a ~ to_submodel(inner1())\n    x ~ Normal(a)\nend\n\nmodel = assume_submodel()",
  "pdb_eight_schools_noncentered": "J = 8\ny = [28, 8, -3, 7, -1, 1, 18, 12]\nsigma = [15, 10, 16, 11, 9, 11, 10, 18]\n\n@model function pdb_eight_schools_noncentered(J, y, sigma)\n    mu ~ Normal(0, 5)\n    tau ~ truncated(Cauchy(0, 5); lower = 0)\n    theta_trans = Vector{Float64}(undef, J)\n    for i = 1:J\n        theta_trans[i] ~ Normal(0, 1)\n        theta = theta_trans[i] * tau + mu\n        y[i] ~ Normal(theta, sigma[i])\n    end\nend\n\nmodel = pdb_eight_schools_noncentered(J, y, sigma)",
  "assume_wishart": "@model function assume_wishart()\n    a ~ Wishart(7, [1.0 0.5; 0.5 1.0])\nend\n\nmodel = assume_wishart()",
  "broadcast_macro": "@model function broadcast_macro(x = [1.5, 2.0], ::Type{TV} = Vector{Float64}) where {TV}\n    a ~ Normal(0, 1)\n    b ~ InverseGamma(2, 3)\n    @. x ~ Normal(a, $(sqrt(b)))\nend\n\nmodel = broadcast_macro()",
  "call_C": "@model function call_C(y = 0.0)\n    x ~ Normal(0, 1)\n\n    # Call C library abs function\n    x_abs = @ccall fabs(x::Cdouble)::Cdouble\n\n    y ~ Normal(0, x_abs)\nend\n\nmodel = call_C()",
  "control_flow": "#= \nThis model illustrates dynamic control flow inside a model that depends on the\nvalue of a random variable. This will cause problems with ReverseDiff's\ncompiled tapes, as a tape compiled at a given value of `a` may not be\nappropriate for a different value of `a`.\n\nTo make sure that the table correctly reflects this issue, the preparation for\nthe gradient is carried out at a value of `a > 0`, and the gradient is\nevaluated at a value of `a < 0`. See `main.jl` for more information.\n=#\n\n@model function control_flow()\n    a ~ Normal()\n    if a > 0\n        b ~ Normal()\n    else\n        b ~ Beta(2, 2)\n    end\nend\n\nmodel = control_flow()"
}