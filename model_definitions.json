{
  "assume_beta": "@model function assume_beta()\n    a ~ Beta(2, 2)\nend\n\nmodel = assume_beta()",
  "assume_dirichlet": "@model function assume_dirichlet()\n    a ~ Dirichlet([1.0, 5.0])\nend\n\nmodel = assume_dirichlet()",
  "demo_assume_dot_observe": "@model function demo_assume_dot_observe(x = [1.5, 2.0])\n    # `assume` and `dot_observe`\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    x .~ Normal(m, sqrt(s))\nend\n\nmodel = demo_assume_dot_observe()",
  "demo_assume_dot_observe_literal": "@model function demo_assume_dot_observe_literal()\n    # `assume` and literal `dot_observe`\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    [1.5, 2.0] .~ Normal(m, sqrt(s))\nend\n\nmodel = demo_assume_dot_observe_literal()",
  "demo_assume_index_observe": "using LinearAlgebra: Diagonal\n\n@model function demo_assume_index_observe(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `assume` with indexing and `observe`\n    s = TV(undef, length(x))\n    for i in eachindex(s)\n        s[i] ~ InverseGamma(2, 3)\n    end\n    m = TV(undef, length(x))\n    for i in eachindex(m)\n        m[i] ~ Normal(0, sqrt(s[i]))\n    end\n    x ~ MvNormal(m, Diagonal(s))\nend\n\nmodel = demo_assume_index_observe()",
  "demo_assume_matrix_observe_matrix_index": "using LinearAlgebra: Diagonal\n\n@model function demo_assume_matrix_observe_matrix_index(\n    x = transpose([1.5 2.0;]),\n    ::Type{TV} = Array{Float64},\n) where {TV}\n    n = length(x)\n    d = n \u00f7 2\n    s ~ reshape(product_distribution(fill(InverseGamma(2, 3), n)), d, 2)\n    s_vec = vec(s)\n    m ~ MvNormal(zeros(n), Diagonal(s_vec))\n    x[:, 1] ~ MvNormal(m, Diagonal(s_vec))\nend\n\nmodel = demo_assume_matrix_observe_matrix_index()",
  "demo_assume_multivariate_observe": "using LinearAlgebra: Diagonal\n\n@model function demo_assume_multivariate_observe(x = [1.5, 2.0])\n    # Multivariate `assume` and `observe`\n    s ~ product_distribution([InverseGamma(2, 3), InverseGamma(2, 3)])\n    m ~ MvNormal(zero(x), Diagonal(s))\n    x ~ MvNormal(m, Diagonal(s))\nend\n\nmodel = demo_assume_multivariate_observe()",
  "demo_assume_multivariate_observe_literal": "using LinearAlgebra: Diagonal\n\n@model function demo_assume_multivariate_observe_literal()\n    # multivariate `assume` and literal `observe`\n    s ~ product_distribution([InverseGamma(2, 3), InverseGamma(2, 3)])\n    m ~ MvNormal(zeros(2), Diagonal(s))\n    [1.5, 2.0] ~ MvNormal(m, Diagonal(s))\nend\n\nmodel = demo_assume_multivariate_observe_literal()",
  "demo_assume_observe_literal": "@model function demo_assume_observe_literal()\n    # univariate `assume` and literal `observe`\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    1.5 ~ Normal(m, sqrt(s))\n    2.0 ~ Normal(m, sqrt(s))\nend\n\nmodel = demo_assume_observe_literal()",
  "demo_assume_submodel_observe_index_literal": "@model function _prior_dot_assume(::Type{TV} = Vector{Float64}) where {TV}\n    s = TV(undef, 2)\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, 2)\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    return s, m\nend\n\n@model function demo_assume_submodel_observe_index_literal()\n    # Submodel prior\n    priors ~ to_submodel(_prior_dot_assume(), false)\n    s, m = priors\n    1.5 ~ Normal(m[1], sqrt(s[1]))\n    2.0 ~ Normal(m[2], sqrt(s[2]))\nend\n\nmodel = demo_assume_submodel_observe_index_literal()",
  "demo_dot_assume_observe": "using LinearAlgebra: Diagonal\n\n@model function demo_dot_assume_observe(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `dot_assume` and `observe`\n    s = TV(undef, length(x))\n    m = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    x ~ MvNormal(m, Diagonal(s))\nend\n\nmodel = demo_dot_assume_observe()",
  "demo_dot_assume_observe_index": "@model function demo_dot_assume_observe_index(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `dot_assume` and `observe` with indexing\n    s = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, length(x))\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    for i in eachindex(x)\n        x[i] ~ Normal(m[i], sqrt(s[i]))\n    end\nend\n\nmodel = demo_dot_assume_observe_index()",
  "assume_lkjcholu": "@model function assume_lkjcholu()\n    a ~ LKJCholesky(5, 1.0, 'U')\nend\n\nmodel = assume_lkjcholu()",
  "demo_dot_assume_observe_index_literal": "@model function demo_dot_assume_observe_index_literal(\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    # `dot_assume` and literal `observe` with indexing\n    s = TV(undef, 2)\n    m = TV(undef, 2)\n    s .~ InverseGamma(2, 3)\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n\n    1.5 ~ Normal(m[1], sqrt(s[1]))\n    2.0 ~ Normal(m[2], sqrt(s[2]))\nend\n\nmodel = demo_dot_assume_observe_index_literal()",
  "demo_dot_assume_observe_matrix_index": "using LinearAlgebra: Diagonal\n\n@model function demo_dot_assume_observe_matrix_index(\n    x = transpose([1.5 2.0;]),\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    s = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, length(x))\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n    x[:, 1] ~ MvNormal(m, Diagonal(s))\nend\n\nmodel = demo_dot_assume_observe_matrix_index()",
  "demo_dot_assume_observe_submodel": "using LinearAlgebra: Diagonal\n\n@model function _likelihood_multivariate_observe(s, m, x)\n    return x ~ MvNormal(m, Diagonal(s))\nend\n\n@model function demo_dot_assume_observe_submodel(\n    x = [1.5, 2.0],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    s = TV(undef, length(x))\n    s .~ InverseGamma(2, 3)\n    m = TV(undef, length(x))\n    m ~ product_distribution(Normal.(0, sqrt.(s)))\n\n    # Submodel likelihood\n    # With to_submodel, we have to have a left-hand side variable to\n    # capture the result, so we just use a dummy variable\n    _ignore ~ to_submodel(_likelihood_multivariate_observe(s, m, x))\nend\n\nmodel = demo_dot_assume_observe_submodel()",
  "dot_assume": "@model function dot_assume(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 5)\n    a .~ Normal()\nend\n\nmodel = dot_assume()",
  "dot_observe": "@model function dot_observe(x = [1.5, 2.0, 2.5])\n    a ~ Normal()\n    x .~ Normal(a)\nend\n\nmodel = dot_observe()",
  "dynamic_constraint": "@model function dynamic_constraint()\n    a ~ Normal()\n    b ~ truncated(Normal(); lower = a)\nend\n\nmodel = dynamic_constraint()",
  "metabayesian_MH": "#=\nThis is a \"meta-Bayesian\" model, where the generative model includes an inversion of a different generative model.\nThese types of models are common in cognitive modelling, where systems of interest (e.g. human subjects) are thought to use Bayesian inference to navigate their environment.\nHere we use a Metropolis-Hasting sampler implemented with Turing as the inversion of the inner \"subjective\" model.\n=#\nusing Random: Xoshiro\n\n# Inner model function\n@model function inner_model(observation, prior_\u03bc = 0, prior_\u03c3 = 1)\n    # The inner model's prior\n    mean ~ Normal(prior_\u03bc, prior_\u03c3)\n    # The inner model's likelihood\n    observation ~ Normal(mean, 1)\nend\n\n# Outer model function\n@model function metabayesian_MH(\n    observation,\n    action,\n    inner_sampler = MH(),\n    inner_n_samples = 20,\n)\n    ### Sample parameters for the inner inference and response ###\n    # The inner model's prior's sufficient statistics\n    subj_prior_\u03bc ~ Normal(0, 1)\n    subj_prior_\u03c3 = 1.0\n    # Inverse temperature for actions\n    \u03b2 ~ Exponential(1)\n\n    ### \"Perceptual inference\": running the inner model ###\n    # Condition the inner model\n    inner_m = inner_model(observation, subj_prior_\u03bc, subj_prior_\u03c3)\n    # Run the inner Bayesian inference\n    chns = sample(Xoshiro(468), inner_m, inner_sampler, inner_n_samples, progress = false)\n    # Extract subjective point estimate\n    subj_mean_expectation\u209c = mean(chns[:mean])\n\n\n    ### \"Response model\": picking an action ###\n    # The action is a Gaussian-noise report of the subjective point estimate\n    action ~ Normal(subj_mean_expectation\u209c, \u03b2)\nend\n\nmodel = metabayesian_MH(0.0, 1.0)",
  "multiple_constraints_same_var": "@model function multiple_constraints_same_var(::Type{TV} = Vector{Float64}) where {TV}\n    x = TV(undef, 5)\n    x[1] ~ Normal()\n    x[2] ~ InverseGamma(2, 3)\n    x[3] ~ truncated(Normal(), -5, 20)\n    x[4:5] ~ Dirichlet([1.0, 2.0])\nend\n\nmodel = multiple_constraints_same_var()",
  "multithreaded": "#=\nMost models in ADTests are run with 1 thread. This model is run with 2 threads\nto properly demonstrate the compatibility with multithreaded observe\nstatements. See `main.jl` for more information.\n=#\n\n@model function multithreaded(x)\n    a ~ Normal()\n    Threads.@threads for i in eachindex(x)\n        x[i] ~ Normal(a)\n    end\nend\n\nmodel = multithreaded([1.5, 2.0, 2.5, 1.5, 2.0, 2.5])",
  "n010": "@model function n010(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 10)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\nmodel = n010()",
  "assume_mvnormal": "@model function assume_mvnormal()\n    a ~ MvNormal([0.0, 0.0], [1.0 0.5; 0.5 1.0])\nend\n\nmodel = assume_mvnormal()",
  "n050": "@model function n050(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 50)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\nmodel = n050()",
  "n100": "@model function n100(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 100)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\nmodel = n100()",
  "n500": "@model function n500(::Type{TV} = Vector{Float64}) where {TV}\n    a = TV(undef, 500)\n    for i in eachindex(a)\n        a[i] ~ Normal()\n    end\nend\n\nmodel = n500()",
  "observe_bernoulli": "@model function observe_bernoulli(x = [true, false, true])\n    a ~ Beta(2, 2)\n    for i in eachindex(x)\n        x[i] ~ Bernoulli(a)\n    end\nend\n\nmodel = observe_bernoulli()",
  "observe_categorical": "@model function observe_categorical(x = [1, 2, 1, 2, 2])\n    a ~ Dirichlet(2, 3)\n    for i in eachindex(x)\n        x[i] ~ Categorical(a)\n    end\nend\n\nmodel = observe_categorical()",
  "observe_index": "@model function observe_index(x = [1.5, 2.0, 2.5])\n    a ~ Normal()\n    for i in eachindex(x)\n        x[i] ~ Normal(a)\n    end\nend\n\nmodel = observe_index()",
  "observe_literal": "@model function observe_literal()\n    a ~ Normal()\n    1.5 ~ Normal(a)\nend\n\nmodel = observe_literal()",
  "observe_multivariate": "@model function observe_multivariate(\n    x = [1.5, 2.0, 2.5],\n    ::Type{TV} = Vector{Float64},\n) where {TV}\n    a = TV(undef, length(x))\n    a .~ Normal()\n    x ~ MvNormal(a, I)\nend\n\nmodel = observe_multivariate()",
  "observe_submodel": "@model function inner2(x, a)\n    x ~ Normal(a)\nend\n@model function observe_submodel(x = 1.5)\n    a ~ Normal()\n    _ignore ~ to_submodel(inner2(x, a))\nend\n\nmodel = observe_submodel()",
  "observe_von_mises": "@model function observe_von_mises(x)\n    a ~ InverseGamma(2, 3)\n    x ~ VonMises(0, a)\nend\n\nmodel = observe_von_mises(0.4)",
  "assume_normal": "@model function assume_normal()\n    a ~ Normal()\nend\n\nmodel = assume_normal()",
  "pdb_eight_schools_centered": "J = 8\ny = [28, 8, -3, 7, -1, 1, 18, 12]\nsigma = [15, 10, 16, 11, 9, 11, 10, 18]\n\n@model function pdb_eight_schools_centered(J, y, sigma)\n    mu ~ Normal(0, 5)\n    tau ~ truncated(Cauchy(0, 5); lower = 0)\n    theta = Vector{Float64}(undef, J)\n    for i = 1:J\n        theta[i] ~ Normal(mu, tau)\n        y[i] ~ Normal(theta[i], sigma[i])\n    end\nend\n\nmodel = pdb_eight_schools_centered(J, y, sigma)",
  "pdb_eight_schools_noncentered": "J = 8\ny = [28, 8, -3, 7, -1, 1, 18, 12]\nsigma = [15, 10, 16, 11, 9, 11, 10, 18]\n\n@model function pdb_eight_schools_noncentered(J, y, sigma)\n    mu ~ Normal(0, 5)\n    tau ~ truncated(Cauchy(0, 5); lower = 0)\n    theta_trans = Vector{Float64}(undef, J)\n    for i = 1:J\n        theta_trans[i] ~ Normal(0, 1)\n        theta = theta_trans[i] * tau + mu\n        y[i] ~ Normal(theta, sigma[i])\n    end\nend\n\nmodel = pdb_eight_schools_noncentered(J, y, sigma)",
  "assume_submodel": "@model function inner1()\n    return a ~ Normal()\nend\n@model function assume_submodel()\n    a ~ to_submodel(inner1())\n    x ~ Normal(a)\nend\n\nmodel = assume_submodel()",
  "assume_wishart": "@model function assume_wishart()\n    a ~ Wishart(7, [1.0 0.5; 0.5 1.0])\nend\n\nmodel = assume_wishart()",
  "broadcast_macro": "@model function broadcast_macro(x = [1.5, 2.0], ::Type{TV} = Vector{Float64}) where {TV}\n    a ~ Normal(0, 1)\n    b ~ InverseGamma(2, 3)\n    @. x ~ Normal(a, $(sqrt(b)))\nend\n\nmodel = broadcast_macro()",
  "call_C": "@model function call_C(y = 0.0)\n    x ~ Normal(0, 1)\n\n    # Call C library abs function\n    x_abs = @ccall fabs(x::Cdouble)::Cdouble\n\n    y ~ Normal(0, x_abs)\nend\n\nmodel = call_C()",
  "control_flow": "#= \nThis model illustrates dynamic control flow inside a model that depends on the\nvalue of a random variable. This will cause problems with ReverseDiff's\ncompiled tapes, as a tape compiled at a given value of `a` may not be\nappropriate for a different value of `a`.\n\nTo make sure that the table correctly reflects this issue, the preparation for\nthe gradient is carried out at a value of `a > 0`, and the gradient is\nevaluated at a value of `a < 0`. See `main.jl` for more information.\n=#\n\n@model function control_flow()\n    a ~ Normal()\n    if a > 0\n        b ~ Normal()\n    else\n        b ~ Beta(2, 2)\n    end\nend\n\nmodel = control_flow()"
}